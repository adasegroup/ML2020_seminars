{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/adasegroup/ML2020_seminars/blob/master/seminar15/seminar_anomaly_detection_for_students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan\n",
    "    \n",
    "* basic examples of anomaly detection with sklearn methods\n",
    " * Robust covariance\n",
    " * One-class SVM\n",
    " * Isolation Forest (and its extended version)\n",
    " * Local Outlier Factor\n",
    " * PCA-based Anomaly Detector\n",
    " * Anomaly detection as classification problem\n",
    " * https://scikit-learn.org/stable/modules/outlier_detection.html\n",
    " \n",
    "* practice & competition https://www.kaggle.com/t/934cc8d07dd649acb9ab40a3386a59d3 - invite link to accept rules (www.kaggle.com/c/ml2020anomalies)\n",
    " * password for archives skml20200309"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Anomaly Detection\n",
    "\n",
    "It's really hard to define anomalies properly. The intuition behind this problem is to find objects that are unexpected in the test data given train sample.\n",
    "\n",
    "One of the approaches is the following:\n",
    "\n",
    "Suppose we have some mechanism of generating data. All data wich came from another mechanism is abnormal.\n",
    "\n",
    "Example:\n",
    " * Normal data from N(0, 1)\n",
    " * Anomaly data from N(0, 10)\n",
    " \n",
    "Note that for some points it almost impossible to destinguish normal and abnormal.\n",
    " \n",
    "Usually we assume that amount of anomaly data is less than normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Framework for Unsupervised Anomaly Detection\n",
    "\n",
    "* Define what is normal.\n",
    "* Calculate deviation from normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_level_lines(model, data, size=100, make_new_figure=True, scatter=True):\n",
    "    if make_new_figure:\n",
    "        plt.figure(figsize=(9, 6))\n",
    "    x_min = data[:, 0].min() - 0.1\n",
    "    x_max = data[:, 0].max() + 0.1\n",
    "    y_min = data[:, 1].min() - 0.1\n",
    "    y_max = data[:, 1].max() + 0.1\n",
    "    all_x = np.linspace(x_min, x_max, size)\n",
    "    all_y = np.linspace(y_min, y_max, size)\n",
    "    XX, YY = np.meshgrid(all_x, all_y)\n",
    "    test_data = np.c_[XX.ravel(), YY.ravel()]\n",
    "    try:\n",
    "        predictions = model.decision_function(test_data).reshape((size, size))\n",
    "        data_scores = model.predict(data)\n",
    "        anomaly_scores = model.decision_function(data)\n",
    "    except AttributeError:\n",
    "        predictions = model._decision_function(test_data).reshape((size, size))\n",
    "        data_scores = model._predict(data)\n",
    "        anomaly_scores = model._decision_function(data)\n",
    "        \n",
    "    plt.contourf(all_x, all_y, predictions)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    threshold = anomaly_scores[data_scores==1.0].min()\n",
    "    plt.contour(XX, YY, predictions, levels=[threshold], linewidths=2, colors='k')\n",
    "\n",
    "    if scatter:\n",
    "        plt.scatter(data[:, 0], data[:, 1], color='k', alpha=0.7)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([x_min,x_max])\n",
    "    axes.set_ylim([y_min,y_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unimodal_data_generator(size=100, anomaly_fraction=0.1):\n",
    "    dim_count = 2\n",
    "    anomaly_size = int(size * anomaly_fraction)\n",
    "    normal_size = size - anomaly_size\n",
    "\n",
    "    linear_transformation = np.array([[2, 0.5],\n",
    "                                      [0.5, 1.5]])\n",
    "    normal_data = np.random.randn(normal_size, dim_count) @ linear_transformation\n",
    "    anomaly_data = np.random.randn(anomaly_size, dim_count)*5\n",
    "    x_min = normal_data[:, 0].min()\n",
    "    x_max = normal_data[:, 0].max()\n",
    "    y_min = normal_data[:, 1].min()\n",
    "    y_max = normal_data[:, 1].max()\n",
    "    data = np.concatenate([normal_data, anomaly_data], axis=0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Covariance\n",
    "\n",
    "Assumes the data is Gaussian and learns an ellipse. Idea: find an ellipsoid of minimal volume with a constraint, namely, that the ellipsoid must contain h points inside its boundary.\n",
    "\n",
    "Pros:\n",
    "* Easy to use\n",
    "* Interpretable\n",
    "* Robust to outliers\n",
    "\n",
    "\n",
    "Cons:\n",
    "* Suitable only for unimodal distribution\n",
    "* Can suffer from ill-posed covariance matrix\n",
    "* May break or not perform well in high-dimensional (when n_samples < n_features ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope\n",
    "np.random.seed(3)\n",
    "data = unimodal_data_generator()\n",
    "\n",
    "model = EllipticEnvelope(assume_centered=True, # Data centering\n",
    "                         contamination=0.1) # Anomalies fraction\n",
    "model.fit(data)\n",
    "plot_level_lines(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Class SVM\n",
    "\n",
    "Try to separate data points from coordinate origin.\n",
    "\n",
    "Pros:\n",
    "* No parametric assumptions\n",
    "* Applicable even for objects not from $R^n$ (because of kernel trick)\n",
    "\n",
    "Cons:\n",
    "* Computationaly difficult\n",
    "* Have to store part of the train data\n",
    "* Sensitive to outliers, thus the training set should not be contaminated by outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generation(size=100, anomaly_fraction=0.1):\n",
    "    dim_count = 2\n",
    "    anomaly_size = int(size * anomaly_fraction)\n",
    "    normal_size = size - anomaly_size\n",
    "    normal_data = make_blobs(normal_size)[0]\n",
    "    anomaly_data = np.random.rand(anomaly_size, dim_count)\n",
    "    x_min = normal_data[:, 0].min()\n",
    "    x_max = normal_data[:, 0].max()\n",
    "    y_min = normal_data[:, 1].min()\n",
    "    y_max = normal_data[:, 1].max()\n",
    "    anomaly_data[:, 0] *= x_max - x_min\n",
    "    anomaly_data[:, 1] *= y_max - y_min\n",
    "    anomaly_data[:, 0] += x_min\n",
    "    anomaly_data[:, 1] += y_min\n",
    "    data = np.concatenate([normal_data, anomaly_data], axis=0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "np.random.seed(1)\n",
    "data = data_generation()\n",
    "\n",
    "model = OneClassSVM(nu=0.3, # Anomalies Fraction\n",
    "                    kernel='rbf', # Kenrel function\n",
    "                    gamma=0.5) # Kernel width\n",
    "model.fit(data)\n",
    "\n",
    "plot_level_lines(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: Let's build a tree which tries to isolate points from the training set. On each iteration we will make a random split. The more splits we need to isolate point, the less abnormal this point is.\n",
    "\n",
    "Pros:\n",
    "\n",
    "* Very robust technique\n",
    "\n",
    "Cons:\n",
    "\n",
    "* Non interpretable\n",
    "* Cannot distinguish concentrated anomalies from normal points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://github.com/adasegroup/ML2020_seminars/raw/master/seminar15/img/isolforest.png \"Isolation Forest\")\n",
    "https://arxiv.org/abs/1811.02141"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "model = IsolationForest(n_estimators=100,\n",
    "                        contamination=0.3, #check difference with 0.1\n",
    "                        max_features=1.0,\n",
    "                        max_samples=1.0,\n",
    "                        bootstrap=True,\n",
    "                        random_state=0)\n",
    "model.fit(data)\n",
    "\n",
    "plot_level_lines(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended Isolation Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://github.com/adasegroup/ML2020_seminars/raw/master/seminar15/img/isolforest_ext.png \"Extended Isolation Forest\")\n",
    "https://arxiv.org/abs/1811.02141"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://github.com/adasegroup/ML2020_seminars/raw/master/seminar15/img/isolforest_cmp.png \"Comparison of Isolation Forests\")\n",
    "https://arxiv.org/abs/1811.02141"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Outlier Factor\n",
    "\n",
    "Idea: usually normal points are highly concentrated whereas bnormal points appear in low density areas.\n",
    "\n",
    "Pros:\n",
    "* No parametric assumptions\n",
    "\n",
    "Cons:\n",
    "* Suffers from the curse of dimensionality\n",
    "* Cannot distinguish concentrated anomalies from normal points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "model = LocalOutlierFactor(n_neighbors=20, \n",
    "                           contamination=0.2,\n",
    "                           metric='minkowski', \n",
    "                           p=2,\n",
    "                           novelty=True)\n",
    "model.fit(data)\n",
    "\n",
    "plot_level_lines(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary & comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://github.com/adasegroup/ML2020_seminars/raw/master/seminar15/img/sphx_glr_plot_anomaly_comparison_001.png \"Comparison of basic methods\")\n",
    "https://scikit-learn.org/stable/auto_examples/plot_anomaly_comparison.html#sphx-glr-auto-examples-plot-anomaly-comparison-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA-based anomaly detection\n",
    "\n",
    "PCA finds projections that maximize explained variance in training data. Let's try to use these projections to find anomalies in test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://github.com/adasegroup/ML2020_seminars/raw/master/seminar15/img/pca_anomaly.png \"PCA based anomaly detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "plt.figure(figsize=(9, 7))\n",
    "np.random.seed(2)\n",
    "X = np.random.randn(200)\n",
    "Y = X + np.random.randn(len(X)) * 0.5\n",
    "model = LinearRegression()\n",
    "model.fit(X.reshape(-1, 1), Y)\n",
    "test_x = np.linspace(-4, 4, 100)\n",
    "predictions = model.predict(test_x.reshape(-1, 1))\n",
    "plt.plot(X, Y, 'o')\n",
    "plt.plot(test_x, predictions, label='No Outliers', color='C0')\n",
    "\n",
    "# Add anomaly\n",
    "sub_inds = np.random.choice(len(X), 10, replace=False)\n",
    "XA = X.copy()\n",
    "YA = Y.copy()\n",
    "YA[sub_inds] = Y[sub_inds] + np.random.rand(len(sub_inds)) * 10\n",
    "model.fit(XA.reshape(-1, 1), YA)\n",
    "predictions = model.predict(test_x.reshape(-1, 1))\n",
    "plt.plot(test_x, predictions, label='With Outliers')\n",
    "plt.plot(XA[sub_inds], YA[sub_inds], 'o', color='C1')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise\n",
    "from sklearn.base import BaseEstimator, OutlierMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class PCAAnomalyDetector(BaseEstimator, OutlierMixin):\n",
    "    def __init__(self, n_components=2, contamination=0.1):\n",
    "        self.n_components = n_components\n",
    "        self.contamination = contamination\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.pca_ = # init and fit PCA\n",
    "        self.offset_ = # calculate contamination-quantile of scores for the training sample (use self.score_samples)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return # rerurn residual vectors\n",
    "        \n",
    "    def score_samples(self, X):\n",
    "        residuals = self.transform(X)\n",
    "        return -np.log1p(np.linalg.norm(residuals, ord=2, axis=1))\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        return # return difference between test scores and self.offest_ (use self.score_samples)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y = np.sign(self.decision_function(X))\n",
    "        return np.asarray(y, dtype=np.intp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCAAnomalyDetector(n_components=1, contamination=0.1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.hstack((X.reshape(-1, 1), Y.reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.hstack((XA.reshape(-1, 1), YA.reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_level_lines(detector, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection as classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "phi = 2\n",
    "S = np.array([[1, 0], \n",
    "              [0, 0.3]])\n",
    "C = np.array([[1, 0], [0, 1.5]])\n",
    "A = S @ np.array([[np.cos(phi), -np.sin(phi)], \n",
    "                  [np.sin(phi), np.cos(phi)]])\n",
    "b = np.array([3, 0])\n",
    "e, r = np.array([1, 0]), 5\n",
    "#e, r = np.array([-0.5, -0.5]), 10 # try\n",
    "#e, r = np.array([-1, -4]), 1 # try\n",
    "\n",
    "X = np.vstack((np.random.randn(3000, 2) @ C,\n",
    "               np.random.randn(100, 2) @ A + b, \n",
    "               r*(np.random.rand(20, 2) + e)))\n",
    "y = np.hstack((np.zeros(3000), np.ones(120)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://github.com/adasegroup/ML2020_seminars/raw/master/seminar15/img/sampling.png \"Sampling techniques\")\n",
    "Undersampling and Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://github.com/adasegroup/ML2020_seminars/raw/master/seminar15/img/smote.png \"SMOTE\")\n",
    "Synthetic Minority Over-sampling Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = RandomUnderSampler(random_state=0)\n",
    "X_resampled, y_resampled = sampler.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_sampler = SMOTE(random_state=0)\n",
    "X_oversampled, y_oversampled = over_sampler.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('y_train counts:', Counter(y_train))\n",
    "print('y_resampled counts:', Counter(y_resampled))\n",
    "print('y_oversampled counts:', Counter(y_oversampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_balanced = LogisticRegression(class_weight='balanced')\n",
    "model = LogisticRegression()\n",
    "model_resampled = LogisticRegression()\n",
    "model_oversampled = LogisticRegression()\n",
    "\n",
    "model_balanced.fit(X_train, y_train)\n",
    "model.fit(X_train, y_train)\n",
    "model_resampled.fit(X_resampled, y_resampled)\n",
    "model_oversampled.fit(X_oversampled, y_oversampled)\n",
    "\n",
    "predictions_balanced = model_balanced.predict_proba(X_test)[:, 1]\n",
    "predictions = model.predict_proba(X_test)[:, 1]\n",
    "predictions_resampled = model_resampled.predict_proba(X_test)[:, 1]\n",
    "predictions_oversampled = model_oversampled.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "models = ['model', 'model_balanced', 'model_resampled', 'model_oversampled']\n",
    "for i in range(len(models)):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plot_level_lines(locals()[models[i]], X_test, make_new_figure=False, scatter=False)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10)\n",
    "    plt.title(models[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = ['predictions', 'predictions_balanced', 'predictions_resampled', 'predictions_oversampled']\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    precision, recall, _ = precision_recall_curve(y_test, locals()[preds[i]])\n",
    "    plt.plot(recall, precision, label='{} APR : {:.3f}'.format(preds[i], average_precision_score(y_test, locals()[preds[i]])))\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
