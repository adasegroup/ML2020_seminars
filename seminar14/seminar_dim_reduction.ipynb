{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/adasegroup/ML2020_seminars/blob/master/seminar14/seminar_dim_reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar: Dimensionality reduction\n",
    "\n",
    "#### Seminar structure:\n",
    "\n",
    "* SVD (Cingular Value Decomposition) and PCA (Principal Component Analysis) from scratch\n",
    "* Singular Value Decomposition of an Image.\n",
    "* The ` Olivetti Faces dataset` component analysis.\n",
    "* Instrinsic dimensionality estimation.\n",
    "* Manifold learning.\n",
    "* Autoencoding.\n",
    "\n",
    "#### Seminar interactive board: https://www.menti.com/b9jckhyeq5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear algebra\n",
    "import numpy as np\n",
    "#data structures\n",
    "import pandas as pd\n",
    "#ml models\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "#plots\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#beautiful plots\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "#off the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SVD (Singular Value Decomposition) from the very scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The singular value decomposition of a matrix $A$ is the factorization of $A$ into the product of three matrices $A = U\\Sigma V^T$ where the columns of $U$ and $V$ are orthonormal and the matrix $\\Sigma$ is diagonal with positive real entries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "X= U\\Sigma V^T=\n",
    "  \\begin{matrix}\n",
    "    \\underbrace{\\left[\\begin{matrix} u_1 &  u_2 &  &  u_r\\end{matrix}\\right.}& \n",
    "    \\underbrace{\\left.\\begin{matrix} u_{r+1} & \\dots &   u_m\\end{matrix}\\right]}\\\\\n",
    "    col(X) & null(X^T)\n",
    "  \\end{matrix}\n",
    "  \\begin{bmatrix}\n",
    "      \\sigma_1 & 0 & \\dots & 0 & 0 & \\dots & 0 \\\\\n",
    "         0 & \\sigma_2  & \\dots & 0 & 0 & \\dots & 0 \\\\\n",
    "         \\dots& & & & &  \\\\\n",
    "         0 & 0 & \\dots & \\sigma_r  & 0 & \\dots & 0 \\\\\n",
    "         0 & 0 & \\dots & 0 & 0 & \\dots & 0 \\\\\n",
    "         \\dots& & & & &  \\\\\n",
    "         0 & 0 & \\dots & 0 & 0 & \\dots & 0 \n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "     v_1^T \\\\  v_2^T \\\\ \\dots \\\\  v_r^T \\\\\n",
    "     v_{r+1}^T \\\\ \\dots \\\\  v_n^T\n",
    "  \\end{bmatrix}\n",
    "  \\begin{matrix}\n",
    "    \\left.\\vphantom{\\begin{bmatrix}\n",
    "       \\ v_1^T \\\\  v_2^T \\\\ \\dots \\\\ \\vect v_r^T \n",
    "       \\end{bmatrix}}\\right\\} row(X) \\\\ \n",
    "    \\left.\\vphantom{\\begin{bmatrix}\n",
    "      \\ v_{r+1}^T \\\\ \\dots \\\\  v_n^T \n",
    "    \\end{bmatrix}}\\right\\} null(X)\n",
    "  \\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://bigdata-madesimple.com/wp-content/uploads/2015/05/61-377x237.png\" alt=\"Drawing\" style=\"width: 300px;\" />\n",
    "Credit for: https://bigdata-madesimple.com/wp-content/uploads/2015/05/61-377x237.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "\n",
    "1. Is `SVD()` use iterative search or there is some closed solution?\n",
    "2. That is the criterion of good decomposition?\n",
    "3. That is the difference of `PCA` and `SVD`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySVD:\n",
    "    \"\"\"\n",
    "    CLass for iterative SVD search\n",
    "    \"\"\"\n",
    "    def __init__(self, tol=1e-10, eigval_thr=1e-10, max_iter=100, random_state=0, verbose=False):\n",
    "        \"\"\"\n",
    "        - tol: tolerance of norm\n",
    "        \n",
    "        \"\"\"\n",
    "        self.tol = tol\n",
    "        self.eigval_thr = eigval_thr\n",
    "        self.max_iter = max_iter\n",
    "        self.gen = np.random.RandomState(random_state)\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def _initialize(self, X, n_components):\n",
    "        self.X = np.array(X)\n",
    "        assert self.X.ndim == 2\n",
    "        # singular values\n",
    "        self.eig_vals = []\n",
    "        # left singular vectors\n",
    "        self.eig_vecs_left = []\n",
    "        # reght singular vectors\n",
    "        self.eig_vecs_right = []\n",
    "        \n",
    "        n_components = self.X.shape[0] if n_components is None else n_components \n",
    "        self.n_components = min(self.X.shape[0], self.X.shape[1], n_components)\n",
    "        \n",
    "    def _make_step(self, X, a, b):\n",
    "        new_b = (b + X.T.dot(a) / np.sum(a**2)) / 2\n",
    "        new_a = (a + X.dot(b) / np.sum(b**2)) / 2\n",
    "\n",
    "        return new_a, new_b\n",
    "        \n",
    "    def __call__(self, X, n_components=None):\n",
    "        self._initialize(X, n_components)\n",
    "        # iteretively search for the components\n",
    "        for n_component in range(self.n_components):\n",
    "            a = ### YOUR CODE HERE ### # lambdas\n",
    "            b = ### YOUR CODE HERE ### # new eigen vecor\n",
    "            \n",
    "            prev_F = ### YOUR CODE HERE ### # calculate Frob norm\n",
    "            delta_F = np.inf\n",
    "            \n",
    "            # stop criterion\n",
    "            for n_iter in range(self.max_iter):\n",
    "                a, b = self._make_step(self.X, a, b)\n",
    "                F = np.sum((self.X - a[:, None] * b[None, :])**2)\n",
    "                delta = prev_F - F\n",
    "                prev_F = F\n",
    "                self._print('n_eig = {}, n_iter = {}: delta = {:.5f}, F = {:.5f}.'.format(\n",
    "                        len(self.eig_vals), n_iter, delta, F))\n",
    "                if np.abs(delta) <= self.tol:\n",
    "                    self._print('Frobenius norm equals {}. Stopping iterations for n_eig={}.'.format(\n",
    "                        F, len(self.eig_vals)))\n",
    "                    break\n",
    "                    \n",
    "            eig_val = np.sum(a**2) * np.sum(b**2) # eigen values\n",
    "            # stop criterion\n",
    "            if eig_val < self.eigval_thr:\n",
    "                self._print('Singular value {} equals {}. Stopping iterations.'.format(\n",
    "                    len(self.eig_vals) + 1, eig_val))\n",
    "                break\n",
    "                \n",
    "            self.eig_vals.append(### YOUR CODE HERE ###) # update eig_vals\n",
    "            self.eig_vecs_left.append(### YOUR CODE HERE ###) # update U\n",
    "            self.eig_vecs_right.append(# update eig_vals) # update V\n",
    "            self.X -= a[:, None] * b[None, :]\n",
    "\n",
    "        self.U = ### YOUR CODE HERE ###\n",
    "        self.V = ### YOUR CODE HERE ###\n",
    "        self.D = ### YOUR CODE HERE ###\n",
    "        return self.U, self.D, self.V.T\n",
    "    \n",
    "    def _print(self, msg):\n",
    "        if self.verbose:\n",
    "            print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = np.random.randn(20, 5)\n",
    "u_np, d_np, v_np = ### YOUR CODE HERE ### # compare with np.linalg\n",
    "u, d, v = MySVD(verbose='True')(test_array)\n",
    "\n",
    "assert np.allclose(d, d_np, atol=1e-2)\n",
    "assert np.allclose(np.dot(np.dot(u, np.diag(d)), v), test_array, atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPCA:\n",
    "    def __init__(self, n_components=10, **svd_kwargs):\n",
    "        self.n_components=n_components\n",
    "        self.svd = MySVD(**svd_kwargs)\n",
    "        \n",
    "    def fit(self, X):\n",
    "        X = np.array(X)\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        \n",
    "        # centering\n",
    "        self.mean_ = X.mean(axis=0)\n",
    "        X = X - self.mean_[None, :]\n",
    "\n",
    "        \n",
    "        # full variance - as a sum of distances to the center\n",
    "        self.full_variance_ = np.sum(np.mean(X**2, axis=0))\n",
    "        \n",
    "        # SVD\n",
    "        self.U, self.D, self.V = ### YOUR CODE HERE ###\n",
    "        \n",
    "        # eigen vectors X^TX in V.T\n",
    "        self.n_components_ = len(self.D)\n",
    "        self.components_ = self.V\n",
    "        \n",
    "        # the potion of variance eplained\n",
    "        self.explained_variance_ = ### YOUR CODE HERE ###\n",
    "        self.explained_variance_ratio_ = ### YOUR CODE HERE ###\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.dot(X - self.mean_[None, :], self.components_.T)\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def recover(self, X):\n",
    "        return np.dot(X, self.components_) + self.mean_[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# toy data\n",
    "plt.figure(figsize=(9, 6))\n",
    "np.random.seed(2)\n",
    "X = np.random.randn(200)\n",
    "Y = X + np.random.randn(len(X)) * 0.5\n",
    "model = LinearRegression()\n",
    "model.fit(X.reshape(-1, 1), Y)\n",
    "test_x = np.linspace(-4, 4, 100)\n",
    "predictions = model.predict(test_x.reshape(-1, 1))\n",
    "plt.plot(X, Y, 'o')\n",
    "plt.plot(test_x, predictions, label='No Outliers', color='C0')\n",
    "\n",
    "# add noise\n",
    "sub_inds = np.random.choice(len(X), 10, replace=False)\n",
    "XA = X.copy()\n",
    "YA = Y.copy()\n",
    "YA[sub_inds] = Y[sub_inds] + np.random.rand(len(sub_inds)) * 10\n",
    "model.fit(XA.reshape(-1, 1), YA)\n",
    "predictions = model.predict(test_x.reshape(-1, 1))\n",
    "plt.plot(test_x, predictions, label='With Outliers')\n",
    "plt.plot(XA[sub_inds], YA[sub_inds], 'o', color='C1')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0.0, 0.0])\n",
    "cov  = np.array([[1.0, -1.0], \n",
    "                [-2.0, 3.0]])\n",
    "\n",
    "#X = np.random.multivariate_normal(mean, cov, 400)\n",
    "X = np.stack([XA,YA]).T\n",
    "\n",
    "pca = ### YOUR CODE HERE ###\n",
    "pca.fit(X)\n",
    "\n",
    "print(\"Explained variance: {}\".format(pca.explained_variance_ratio_))\n",
    "print(\"Principal components:\")\n",
    "print(\"First: {}\".format(pca.components_[0]))\n",
    "print(\"Second: {}\".format(pca.components_[0]))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=25, c='r', marker='o', edgecolor='k')\n",
    "\n",
    "for var_ratio, direction in zip(pca.explained_variance_ratio_, pca.components_):\n",
    "    d = 7 * np.sqrt(var_ratio) * direction\n",
    "    plt.plot([0, d[0]], [0, d[1]], '-k', lw=2)\n",
    "\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Watch in the next series (seminar): \"Anomaly detection with PCA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Singular Value Decomposition of an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/adasegroup/ML2020_seminars/raw/master/seminar14/img/think-about-it.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/adasegroup/ML2020_seminars/raw/master/seminar14/img/Spongebob.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/adasegroup/ML2020_seminars/raw/master/seminar14/img/realization.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library for images\n",
    "from PIL import Image\n",
    "\n",
    "img = Image.open('think-about-it.png')\n",
    "imggray = img.convert('LA')\n",
    "plt.figure(figsize=(9, 6));\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgmat = np.array(list(imggray.getdata(band=0)), float)\n",
    "imgmat.shape = (imggray.size[1], imggray.size[0])\n",
    "imgmat = np.matrix(imgmat)\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.imshow(imgmat, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can try this one, yet it converges much longer\n",
    "\n",
    "# U, sigma, V = MySVD(verbose=False)(imgmat)  \n",
    "U, sigma, V = np.linalg.svd(imgmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing an approximation of the image using the first column of  $U$  and first row of  $V$  reproduces the most prominent feature of the image, the light area on top and the dark area on the bottom. The darkness of the arch causes the extra darkness in the middle of the reconstruction. Each column of pixels in this image is a different weighting of the same values,  $u_1$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstimg = ### YOUR CODE HERE ###\n",
    "plt.imshow(reconstimg, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we will be able to see the appearance from 2-m component?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 4):\n",
    "    reconstimg = ### YOUR CODE HERE ###\n",
    "    plt.imshow(reconstimg, cmap='gray')\n",
    "    title = \"n = %s\" % i\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "print(' Soooo clooosee .....')    \n",
    "plt.imshow(Image.open('Spongebob.png'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, how many do we need?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok how many do we need\n",
    "for i in range(0, 51, 5):\n",
    "    reconstimg = ### YOUR CODE HERE ###\n",
    "    plt.imshow(reconstimg, cmap='gray')\n",
    "    title = \"n = %s\" % i\n",
    "    plt.title(title)\n",
    "    plt.show() \n",
    "plt.imshow(Image.open('realization.jpg'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For my perspective, `[:45]` reconstruction os good enough. Thus we can store less data.**\n",
    "\n",
    "Questions: \n",
    "* What portion of the data we can discard this way?\n",
    "* Can we estimate the intrinsic dimension of the image through PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA().fit(imgmat)\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "plt.step(range(len(pca.components_)), ### YOUR CODE HERE ###, label='cumulative explained variance')\n",
    "plt.title('Cumulative explained variance', fontsize=16)\n",
    "plt.xlabel('# principle components', fontsize=12)\n",
    "plt.ylabel('Cumulative explained variance', fontsize=12)\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.hlines(y=0.95, xmin=0, xmax= len(pca.components_), colors='r', linestyles='dashed', label='95% explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The `Olivetti Faces dataset` component analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from numpy.random import RandomState\n",
    "\n",
    "\n",
    "data = fetch_olivetti_faces(shuffle=True, random_state= 42 ).data\n",
    "target = fetch_olivetti_faces(shuffle=True, random_state= 42).target\n",
    "\n",
    "#data\n",
    "plt.matshow(data[0].reshape(64,64), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, sigma, V = np.linalg.svd(data)\n",
    "\n",
    "plt.matshow(V[:,150].reshape(64,64), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "* Who is that criminally looking guy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA().fit(data)\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "plt.step(### YOUR CODE HERE ###, label='cumulative explained variance')\n",
    "plt.title('Cumulative explained variance', fontsize=16)\n",
    "plt.xlabel('# principle components', fontsize=12)\n",
    "plt.ylabel('Cumulative explained variance', fontsize=12)\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.hlines(y=0.95, xmin=0, xmax=len(pca.components_), colors='r', linestyles='dashed', label='95% explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import offsetbox\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "\n",
    "def plot_embedding(X, y, images_small=None, title=None):\n",
    "    \"\"\"\n",
    "    Nice plot on first two components of embedding with Offsets.\n",
    "    \n",
    "    \"\"\"\n",
    "    # take only first two columns\n",
    "    X = X[:, :2]\n",
    "    # scaling\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "    plt.figure(figsize=(13,8))\n",
    "    ax = plt.subplot(111)\n",
    "    \n",
    "    for i in range(X.shape[0] - 1):\n",
    "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
    "                 color=plt.cm.RdGy(y[i]),\n",
    "                 fontdict={'weight': 'bold', 'size': 12})\n",
    "        if images_small is not None:\n",
    "            imagebox = OffsetImage(images_small[i], zoom=.4, cmap = 'gray')\n",
    "            ab = AnnotationBbox(imagebox, (X[i, 0], X[i, 1]),\n",
    "                xycoords='data')                                  \n",
    "            ax.add_artist(ab)\n",
    "    \n",
    "    if hasattr(offsetbox, 'AnnotationBbox'):\n",
    "        # only print thumbnails with matplotlib > 1.0\n",
    "        shown_images = np.array([[1., 1.]])  \n",
    "        for i in range(X.shape[0]):\n",
    "            dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "            if np.min(dist) < 4e-1:\n",
    "                # don't show points that are too close\n",
    "                continue\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_projected = ### YOUR CODE HERE ###\n",
    "data_pic = data.reshape((-1, 64, 64))\n",
    "plot_embedding(X_projected, target, data_pic, \"PCA decomposition, projection on first two components  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I will change the number of components in here `PCA(100)` will it change the manifold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Estimation of intrinsic dimension\n",
    "\n",
    "From the paper: \n",
    "\n",
    "\n",
    "*There is a consensus in the high-dimensional data analysis community that the only reason any methods work in very high dimensions is that, in fact, the data are not truly high-dimensional. Rather, they are embedded in a high-dimensional space,\n",
    "but can be efficiently summarized in a space of a much lower dimension, such as a nonlinear manifold.* \n",
    "\n",
    "*Then one can reduce dimension without losing much information for many types of real-life high-dimensional data, such as images, and avoid many of the “curses of dimensionality”. Learning these data manifolds can improve\n",
    "performance in classification and other applications, but if the data structure is\n",
    "complex and nonlinear, dimensionality reduction can be a hard problem.*\n",
    "\n",
    "#####  NIPS 2004: https://papers.nips.cc/paper/2577-maximum-likelihood-estimation-of-intrinsic-dimension.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of 'Maximum Likelihood Estimation of Intrinsic Dimension' by Elizaveta Levina and Peter J. Bickel\n",
    " \n",
    "how to use\n",
    "----------\n",
    " \n",
    "The goal is to estimate intrinsic dimensionality of data, the estimation of dimensionality is scale dependent\n",
    "(depending on how much you zoom into the data distribution you can find different dimesionality), so they\n",
    "propose to average it over different scales, the interval of the scales [k1, k2] are the only parameters of the algorithm.\n",
    " \n",
    "This code also provides a way to repeat the estimation with bootstrapping to estimate uncertainty.\n",
    " \n",
    "Here is one example with swiss roll :\n",
    " \n",
    "from sklearn.datasets import make_swiss_roll\n",
    "X, _ = make_swiss_roll(1000)\n",
    " \n",
    "k1 = 10 # start of interval(included)\n",
    "k2 = 20 # end of interval(included)\n",
    "intdim_k_repeated = repeated(intrinsic_dim_scale_interval, \n",
    "                             X, \n",
    "                             mode='bootstrap', \n",
    "                             nb_iter=500, # nb_iter for bootstrapping\n",
    "                             verbose=1, \n",
    "                             k1=k1, k2=k2)\n",
    "intdim_k_repeated = np.array(intdim_k_repeated)\n",
    "# the shape of intdim_k_repeated is (nb_iter, size_of_interval) where \n",
    "# nb_iter is number of bootstrap iterations (here 500) and size_of_interval\n",
    "# is (k2 - k1 + 1).\n",
    " \n",
    "\"\"\"\n",
    "# from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    " \n",
    "def intrinsic_dim_sample_wise(X, k=5):\n",
    "    neighb = NearestNeighbors(n_neighbors=k+1).fit(X)\n",
    "    dist, ind = neighb.kneighbors(X) # distances between the samples and points\n",
    "    dist = dist[:, 1:] # the distance between the first points to first points (as basis ) equals zero\n",
    "    # the first non trivial point\n",
    "    dist = dist[:, 0:k]# including points k-1\n",
    "    assert dist.shape == (X.shape[0], k) # requirments are there is no equal points\n",
    "    assert np.all(dist > 0)\n",
    "    d = np.log(dist[:, k - 1: k] / dist[:, 0:k-1]) # dinstance betveen the bayeasan statistics\n",
    "    d = d.sum(axis=1) / (k - 2)\n",
    "    d = 1. / d\n",
    "    intdim_sample = d\n",
    "    return intdim_sample\n",
    " \n",
    "def intrinsic_dim_scale_interval(X, k1=10, k2=20):\n",
    "    X = pd.DataFrame(X).drop_duplicates().values # remove duplicates in case you use bootstrapping\n",
    "    intdim_k = []\n",
    "    for k in range(k1, k2 + 1): # in order to reduse the noise by eliminating of the nearest neibours \n",
    "        m = intrinsic_dim_sample_wise(X, k).mean()\n",
    "        intdim_k.append(m)\n",
    "    return intdim_k\n",
    " \n",
    "def repeated(func, X, nb_iter=100, random_state=None, mode='bootstrap', **func_kw):\n",
    "    if random_state is None:\n",
    "        rng = np.random\n",
    "    else:\n",
    "        rng = np.random.RandomState(random_state)\n",
    "    nb_examples = X.shape[0]\n",
    "    results = []\n",
    " \n",
    "    iters = range(nb_iter) \n",
    "    for i in iters:\n",
    "        if mode == 'bootstrap':# and each point we want to resample with repeating points to reduse the errors \n",
    "            #232 111 133 \n",
    "            Xr = X[rng.randint(0, nb_examples, size=nb_examples)]\n",
    "        elif mode == 'shuffle':\n",
    "            ind = np.arange(nb_examples)\n",
    "            rng.shuffle(ind)\n",
    "            Xr = X[ind]\n",
    "        elif mode == 'same':\n",
    "            Xr = X\n",
    "        else:\n",
    "            raise ValueError('unknown mode : {}'.format(mode))\n",
    "        results.append(func(Xr, **func_kw))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "k1 = # start of interval(included)\n",
    "k2 = # end of interval(included)\n",
    "nb_iter = # more iterations more accuracy\n",
    "# intrinsic_dim_scale_interval gives better estimation\n",
    "intdim_k_repeated = repeated(intrinsic_dim_scale_interval, \n",
    "                             ### YOUR CODE HERE ###, \n",
    "                             mode='bootstrap', \n",
    "                             nb_iter=nb_iter, # nb_iter for bootstrapping\n",
    "                             k1=k1, k2=k2)\n",
    "intdim_k_repeated = np.array(intdim_k_repeated)\n",
    "\n",
    "x = np.arange(k1, k2+1)\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(x, np.mean(intdim_k_repeated, axis=0), 'b', label='Mean') # it is the mean walue\n",
    "plt.fill_between(x, \n",
    "                 np.mean(intdim_k_repeated, axis=0) - \n",
    "                 2 * np.std(intdim_k_repeated, axis=0),\n",
    "                 np.mean(intdim_k_repeated, axis=0) + \n",
    "                 2 * np.std(intdim_k_repeated, axis=0),\n",
    "                 alpha=0.3,\n",
    "                 label='CI=95%',\n",
    "                 color='g')\n",
    "plt.xlabel(\"Nearest Neigbours\")\n",
    "plt.ylabel(\"Intrinsic Dimensionality\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Manifold learning  \n",
    "\n",
    "It is big part of research on the application of geometry and in particular differential geometry to machine learning. If you are interested - next term it will be course by **Prof. Bernstein** - you are welcomed. \n",
    "\n",
    "We are going to touch a couple popular algorithms from: https://scikit-learn.org/stable/modules/manifold.html\n",
    "\n",
    "The manifold learning methods also assumes nono-linear algorithms for dimensionality reduction.\n",
    "\n",
    "Questions:\n",
    "* Is `PCA` linear?\n",
    "* Can we compose non-linear `PCA` from the linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA), Independent Component Analysis, Linear Discriminant Analysis, and others. These algorithms define specific rubrics to choose an good linear projection of the data. \n",
    "\n",
    "These methods can be powerful, but often miss important non-linear structure in the data.\n",
    "\n",
    "\n",
    "**Lets go with some other liner method : Independent Component Analysis `ICA`.** \n",
    "FastICA algorithm: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cf2.ppt-online.org/files2/slide/c/CarDgVY6t91bypGImvqBcR3OdAjWhFTZ2JQxL4ofM/slide-6.jpg\" alt=\"Drawing\" style=\"width: 700px;\" />\n",
    "Credit for: https://cf2.ppt-online.org/files2/slide/c/CarDgVY6t91bypGImvqBcR3OdAjWhFTZ2JQxL4ofM/slide-6.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's leave 20 peaple from faces to get more comprehencible visualisation\n",
    "\n",
    "data = fetch_olivetti_faces(shuffle=True, random_state= 42 ).data\n",
    "target = fetch_olivetti_faces(shuffle=True, random_state= 42).target\n",
    "\n",
    "\n",
    "data = data[### YOUR CODE HERE ###]\n",
    "target = target[### YOUR CODE HERE ###]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA \n",
    "\n",
    "X_projected = ### YOUR CODE HERE ###\n",
    "data_pic = data.reshape((-1, 64, 64))\n",
    "plot_embedding(X_projected, target, data_pic, \"ICA decomposition, projection on two components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will the decomposition change with the `n_components` changed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The most popular nonlinear algorithms: tSNE,  MDS, Isomap**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.\n",
    "\n",
    "1. For each data point $x_i$ we’ll center a Gaussian distribution over that point. Then we measure the density of all points $x_j$ under that Gaussian distribution. Then renormalize for all points. This gives us a set of probabilities $P_{ij}$ for all points in higher dimentional space.\n",
    "\n",
    "2. Get the second set of probablities $Q_{ij}$ for Cauchy distribution (with is Students t-distribution with one degree of freedom), which allow for better modeling of far apart distances, becouse of heavier tails.\n",
    "\n",
    "3. Map these two sets of probabilities ($ P_{ij}; Q_{ij}$)to each other, optimiing KL-divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where Kullback–Leibler divergence can be defined as \n",
    "\n",
    "$$ D_{KL} (P || Q) = \\sum P(x) log(\\frac{P(x)}{Q(x)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "tsne = ### YOUR CODE HERE ###\n",
    "\n",
    "X_projected = tsne.fit_transform(data)\n",
    "data_pic = data.reshape((-1, 64, 64))\n",
    "plot_embedding(X_projected, target, data_pic, \"tSNE decomposition, projection on two components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isometric Mapping (Isomap)\n",
    "\n",
    "Isomap can be viewed as an extension of Multi-dimensional Scaling (MDS) or Kernel PCA. Isomap seeks a lower-dimensional embedding which maintains geodesic distances between all points. Goes in three stages:\n",
    "\n",
    "1. **Nearest neighbor search**. \n",
    "\n",
    "2. **Shortest-path graph search.**\n",
    "\n",
    "3. **Partial eigenvalue decomposition**. The embedding is encoded in the eigenvectors corresponding to the  largest eigenvalues of the  isomap kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "\n",
    "X_projected = ### YOUR CODE HERE ###\n",
    "data_pic = data.reshape((-1, 64, 64))\n",
    "plot_embedding(X_projected, target, data_pic, \"Isomap decomposition, projection on  two components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-dimensional Scaling (MDS)\n",
    "\n",
    "Multidimensional scaling (MDS) seeks a low-dimensional representation of the data in which the distances respect well the distances in the original high-dimensional space.\n",
    "\n",
    "*Modern Multidimensional Scaling - Theory and Applications, Borg, I.; Groenen P. Springer Series in Statistics (1997)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "\n",
    "X_projected = ### YOUR CODE HERE ###\n",
    "data_pic = data.reshape((-1, 64, 64))\n",
    "plot_embedding(X_projected, target, data_pic, \"MDS decomposition, projection on two components\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
