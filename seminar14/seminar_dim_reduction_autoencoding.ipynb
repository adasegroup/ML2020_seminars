{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/adasegroup/ML2020_seminars/blob/master/seminar14/seminar_dim_reduction_autoencoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar: Dimensionality reduction. Autoencoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Autoencoders__ or how it was on the lectures - replicational neural networks - are unsupervised artificial neural network that learns how to efficiently compress and encode data then learns how to reconstruct the data back from the reduced encoded representation to a representation that is as close to the original input as possible.\n",
    "\n",
    "It is good for:\n",
    "* redusing noise in the data\n",
    "* building stable networks for data invariands\n",
    "* data compression\n",
    "\n",
    "The encoder brings the data from a high dimensional input to a __Latent space__ or *bottleneck* layer, where the number of neurons is the smallest. Then, the decoder takes this encoded input and converts it back to the original input shape — in our case an image. The latent space is the space in which the data lies in the bottleneck layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.compthree.com/images/blog/ae/ae.png\" alt=\"Drawing\" style=\"width: 700px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as torch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data = fetch_olivetti_faces(shuffle=True, random_state= 42 ).data\n",
    "target = fetch_olivetti_faces(shuffle=True, random_state= 42).target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, stratify=target, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "#data\n",
    "plt.matshow(data[0].reshape(64,64), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Write `torch` compatible dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacesData(torch_data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(FacesData, self).__init__()\n",
    "        self.X = ### YOUR CODE HERE ###                                                    \n",
    "        self.y = ### YOUR CODE HERE ###\n",
    "    \n",
    "    def __len__(self):\n",
    "        return ### YOUR CODE HERE ###\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = FacesData(X_train, y_train) \n",
    "test_dset = FacesData(X_test, y_test) \n",
    "\n",
    "print(train_dset[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Write Autoencoder model as object of  `torch.nn.Module` class\n",
    "\n",
    "It takes as input encoder and decoder (it will be small neural networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFirstAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(MyFirstAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Take a mini-batch as an input, encode it to the latent space and decode back to the original space\n",
    "        x_out = decoder(encoder(x))\n",
    "        :param x: torch.tensor, (MB, x_dim)\n",
    "        :return: torch.tensor, (MB, x_dim)\n",
    "        \"\"\"\n",
    "        z = ### YOUR CODE HERE ###\n",
    "        x_out = ### YOUR CODE HERE ###\n",
    "        return x_out #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define Encoder and Decoder, whey will be symmetrical\n",
    "\n",
    "You should define variable for bottelneck layer - `hid` and for number of neurons in the whole network  - `ss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = 128\n",
    "\n",
    "samples, sample_size = data.shape\n",
    "\n",
    "encoder = lambda hid: nn.Sequential(\n",
    "                        ### YOUR CODE HERE ###\n",
    "                        )  \n",
    "\n",
    "decoder =  lambda hid: nn.Sequential(\n",
    "                        ### YOUR CODE HERE ###\n",
    "                        ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Defining criterion, optimizer, scheduler and data loaders\n",
    "\n",
    "Choose criterion, it will be `nnMSEloss` for now, optimizer and scheduler for optimiser\n",
    "\n",
    "Quiestion: why do we need the scheduler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = 'cuda:0'  ### YOUR CODE HERE ###\n",
    "device = 'cpu'\n",
    "\n",
    "net = MyFirstAE(encoder(50), decoder(50))  \n",
    "criterion = nn.MSELoss()\n",
    "optimizer = ### YOUR CODE HERE ###\n",
    "scheduler = ### YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "train_loader = ### YOUR CODE HERE ###\n",
    "val_loader = ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. The main part - write `train` for the network\n",
    "\n",
    "Train will take the batch, send to the devise, encode and decode it and calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, net, criterion, optimizer, train_loader, val_loader,scheduler=None, verbose=True, save_dir=None):\n",
    "    net.to(device)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        net.train()\n",
    "        for X, _ in train_loader:\n",
    "             ### YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "        net.eval()\n",
    "        for X, _ in val_loader:\n",
    "             ### YOUR CODE HERE ###\n",
    "         \n",
    "         \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        freq = max(epochs//20,1)\n",
    "        if verbose and epoch%freq==0:\n",
    "            print('Epoch {}/{} || Loss:  Train {:.4f} | Validation {:.4f}'.format(epoch, epochs, loss.item(), val_loss.item()))\n",
    "            \n",
    "    if save_dir is not None:\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, 'model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Enjoy the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for `MSE` loss lets get < 0.011 on validation, with AE \"bottleneck\" = 50\n",
    "\n",
    "train(300, net, criterion, optimizer, train_loader, val_loader, scheduler)  ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ### it was for AE with ss = 128, epochs = 500, bottleneck = 50, MSE = 0.011 val\n",
    "fig, ax = plt.subplots(ncols=10, nrows=2, figsize=(20, 5))\n",
    "\n",
    "n_pics = 64\n",
    "\n",
    "for i in range(10):\n",
    "    im = train_dset[i][0]\n",
    "    rec = ### YOUR CODE HERE ###\n",
    "    ax[0, i].imshow(im[0].reshape(n_pics,n_pics), cmap = \"gray\");\n",
    "    ax[1, i].imshow(rec.detach().cpu().reshape(n_pics,n_pics), cmap = \"gray\");\n",
    "    ax[0, i].axis('off')\n",
    "    ax[1, i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import offsetbox\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "\n",
    "def plot_embedding(X, y, images_small=None, title=None):\n",
    "    \"\"\"\n",
    "    Nice plot on first two components of embedding with Offsets.\n",
    "    \n",
    "    \"\"\"\n",
    "    # take only first two columns\n",
    "    X = X[:, :2]\n",
    "    # scaling\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "    plt.figure(figsize=(13,8))\n",
    "    ax = plt.subplot(111)\n",
    "    \n",
    "    for i in range(X.shape[0] - 1):\n",
    "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
    "                 color=plt.cm.RdGy(y[i]),\n",
    "                 fontdict={'weight': 'bold', 'size': 12})\n",
    "        if images_small is not None:\n",
    "            imagebox = OffsetImage(images_small[i], zoom=.4, cmap = 'gray')\n",
    "            ab = AnnotationBbox(imagebox, (X[i, 0], X[i, 1]),\n",
    "                xycoords='data')                                  \n",
    "            ax.add_artist(ab)\n",
    "    \n",
    "    if hasattr(offsetbox, 'AnnotationBbox'):\n",
    "        # only print thumbnails with matplotlib > 1.0\n",
    "        shown_images = np.array([[1., 1.]])  \n",
    "        for i in range(X.shape[0]):\n",
    "            dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "            if np.min(dist) < 4e-1:\n",
    "                # don't show points that are too close\n",
    "                continue\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "X_projected = PCA(50).fit_transform(data)\n",
    "data_pic = data.reshape((-1, 64, 64))\n",
    "plot_embedding(X_projected, target, data_pic, \"PCA. Projection on two components out of 50 first \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_projected = ### YOUR CODE HERE ###\n",
    "data_pic = data.reshape((-1, 64, 64))\n",
    "plot_embedding(X_projected, target, data_pic, \"PCA. AE with 50 latent features, MSE = 0.011\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "* What we can derive from this embedding visually?\n",
    "\n",
    "* How to compare the effectiveness of these embeddings?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
